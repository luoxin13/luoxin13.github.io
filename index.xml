<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xin Luo&#39;s Blog</title>
    <link>https://luoxin13.github.io/</link>
    <description>Recent content on Xin Luo&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Apr 2022 11:29:02 +0800</lastBuildDate><atom:link href="https://luoxin13.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
    <title>元学习</title>
    <link>https://luoxin13.github.io/posts/notes-on-meta-learning/</link>
    <pubDate>Thu, 07 Apr 2022 11:29:02 +0800</pubDate>
    
    <guid>https://luoxin13.github.io/posts/notes-on-meta-learning/</guid>
    <description>
        &lt;h2 id=&#34;什么是元学习&#34;&gt;
    &lt;a href=&#34;#%e4%bb%80%e4%b9%88%e6%98%af%e5%85%83%e5%ad%a6%e4%b9%a0&#34; class=&#34;anchor&#34;&gt;
        &lt;svg class=&#34;icon&#34; aria-hidden=&#34;true&#34; focusable=&#34;false&#34; height=&#34;16&#34; version=&#34;1.1&#34; viewBox=&#34;0 0 16 16&#34; width=&#34;16&#34;&gt;
            &lt;path fill-rule=&#34;evenodd&#34;
                d=&#34;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&#34;&gt;
            &lt;/path&gt;
        &lt;/svg&gt;
    &lt;/a&gt;
    什么是元学习？
&lt;/h2&gt;
&lt;p&gt;进入智能化时代，人工智能的终极目标也正是让机器拥有人的智能。在人类拥有的众多能力中，最重要的能力当属&lt;strong&gt;学习能力&lt;/strong&gt;，正是学习能力让人类能够不断掌握新的知识和技能，支持生命活动。因此，让机器&lt;strong&gt;学会学习&lt;/strong&gt;是实现人工智能的主要目标，在机器学习中出现的&lt;strong&gt;元学习&lt;/strong&gt;概念正试图实现这一目标。元学习由不同层次的学习抽象而成，使得目标机器学习系统能够通过学习自己的主要组件（例如优化器、损失函数、初始化方式、模型架构等）对自己的学习能力进行改进，故而元学习也被称为&amp;quot;&lt;strong&gt;学会如何学习 (learning to learn)&lt;/strong&gt;&amp;quot;。&lt;/p&gt;
&lt;p&gt;元学习通常将学习过程抽象为两个或更多层次：在最内层，模型学习任务相关的知识（例如在新的数据集上对模型进行精调）；在最外层，模型则需要学习&amp;quot;跨任务&amp;quot;知识（例如通过学习在不同任务之间进行迁移）。若最内层的组件存在可学习参数，则最外层优化时可以通过对这些组件进行&amp;quot;元学习&amp;quot;，进而能够对这些最内层的组件进行自动化学习。&lt;/p&gt;
&lt;h2 id=&#34;模型无关的元学习-brmaml-model-agnostic-meta-learning&#34;&gt;
    &lt;a href=&#34;#%e6%a8%a1%e5%9e%8b%e6%97%a0%e5%85%b3%e7%9a%84%e5%85%83%e5%ad%a6%e4%b9%a0-brmaml-model-agnostic-meta-learning&#34; class=&#34;anchor&#34;&gt;
        &lt;svg class=&#34;icon&#34; aria-hidden=&#34;true&#34; focusable=&#34;false&#34; height=&#34;16&#34; version=&#34;1.1&#34; viewBox=&#34;0 0 16 16&#34; width=&#34;16&#34;&gt;
            &lt;path fill-rule=&#34;evenodd&#34;
                d=&#34;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&#34;&gt;
            &lt;/path&gt;
        &lt;/svg&gt;
    &lt;/a&gt;
    模型无关的元学习 &lt;/br&gt;MAML, Model-Agnostic Meta-Learning
&lt;/h2&gt;
&lt;p&gt;MAML 是一个经典的元学习框架，模型学习神经网络的参数 $\theta$（初始化为 $\theta=\theta_0$）。针对特定任务的支持集（Support Set）$\mathcal{S}={x_S,y_S}$，元学习先利用较少次数（$N=1\ldots 5$）的标准SGD算法对模型进行优化，然后通过二次学习得到对任务的目标集（Target Set）$\mathcal{T}={x_T,y_T}$ 泛化性能较好的模型参数。&lt;/p&gt;
&lt;p&gt;具体来说，在元学习任务中，给定一个任务，任务由支持集和目标集两个集合组成，其中的支持集由若干批量的输入-输出对（${x_S,y_S}$）组成，目标集则是一个小数量的验证集合，同样由输入-输出对（${x_T,y_T}$）组成。任务开始时，元学习得到初始化的模型参数 $\theta_0=\theta$。然后，开始进行内层循环的优化（inner loop optimization）：从第一个步骤开始，在每个步骤 $i$， 神经网络 $f$ 的参数设置为 $\theta_{i-1}$，接受此时的支持集作为输入，输出当前的预测结果 $f(x_s;\theta_{i-1})$，将该预测结果与支持集的标签一起，由定义的损失函数 $L$， 计算得到支持集的损失 $L_{i-1}^S$；此时，可用当前的损失对应的梯度，由 SGD 算法更新模型参数，得到当前步骤的模型参数 $\theta_{i}=\theta_{i-1}-\alpha$&lt;/p&gt;

    </description>
    </item>
    
  </channel>
</rss>
